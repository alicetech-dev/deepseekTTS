{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alicetech-dev/deepseekTTS/blob/main/deepseekTTS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPHR1d1Nvk7W",
        "outputId": "d651e003-05e3-44bd-848a-3556f8265ec1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "# Download and run the Ollama Linux install script\n",
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGYQyFhtvk7c"
      },
      "outputs": [],
      "source": [
        "# Get Ngrok authentication token from colab secrets environment\n",
        "#obten tu token de Ngrok y agregalo a tu secrets de google colab con el nombre NGROK_AUT_TOKEN\n",
        "from google.colab import userdata\n",
        "NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDNgwkemvk7f",
        "outputId": "241f76bd-c04b-439f-9234-48d544a6fe59",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.11)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.3-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.11/dist-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Downloading pyngrok-7.2.3-py3-none-any.whl (23 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.3\n",
            ">>> starting ngrok config add-authtoken 2sMBQ87UvsxOFUdrhamt8RPRTfL_4PU5WQrifsQDbxbfnsmJR\n",
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "# Install:\n",
        "#  1. aiohttp for concurrent subprocess execution in Jupyter Notebooks\n",
        "#  2. pyngrok for Ngrok wrapper\n",
        "!pip install aiohttp pyngrok\n",
        "\n",
        "import asyncio\n",
        "import os\n",
        "\n",
        "# Set LD_LIBRARY_PATH so the system NVIDIA library becomes preferred\n",
        "# over the built-in library. This is particularly important for\n",
        "# Google Colab which installs older drivers\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "# Define run - a helper function to run subcommands asynchronously.\n",
        "# The function takes in 2 arguments:\n",
        "#  1. command\n",
        "#  2. environment variable\n",
        "async def run(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE\n",
        "  )\n",
        "\n",
        "\n",
        "# This function is designed to handle large amounts of text data efficiently.\n",
        "# It asynchronously iterate over lines and print them, stripping and decoding as needed.\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "\n",
        "# Gather the standard output (stdout) and standard error output (stderr) streams of a subprocess and pipe them through\n",
        "# the `pipe()` function to print each line after stripping whitespace and decoding UTF-8.\n",
        "# This allows us to capture and process both the standard output and error messages from the subprocess concurrently.\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "\n",
        "# Authenticate with Ngrok\n",
        "await asyncio.gather(\n",
        "  run(['ngrok', 'config', 'add-authtoken', NGROK_AUTH_TOKEN])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lT81i7bCvk7i",
        "outputId": "c285cdeb-0204-43e7-8499-ae2ec9792389",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> starting ollama serve\n",
            ">>> starting ngrok http --log stderr 11434 --host-header localhost:11434 --domain moccasin-just-pipefish.ngrok-free.app\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"no configuration paths supplied\"\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"using configuration at default config path\" path=/root/.config/ngrok/ngrok.yml\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"open config file\" path=/root/.config/ngrok/ngrok.yml err=nil\n",
            "Couldn't find '/root/.ollama/id_ed25519'. Generating new private key.\n",
            "Your new public key is:\n",
            "\n",
            "ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIMjH6XCPqbW80weESHsZBMdEKZrziA9EHu3NywvfVDVp\n",
            "\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"starting web service\" obj=web addr=127.0.0.1:4040 allow_hosts=[]\n",
            "2025/01/31 00:31:29 routes.go:1187: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: HTTPS_PROXY: HTTP_PROXY: NO_PROXY: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_GPU_OVERHEAD:0 OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE:5m0s OLLAMA_KV_CACHE_TYPE: OLLAMA_LLM_LIBRARY: OLLAMA_LOAD_TIMEOUT:5m0s OLLAMA_MAX_LOADED_MODELS:0 OLLAMA_MAX_QUEUE:512 OLLAMA_MODELS:/root/.ollama/models OLLAMA_MULTIUSER_CACHE:false OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:0 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://* vscode-webview://*] OLLAMA_SCHED_SPREAD:false ROCR_VISIBLE_DEVICES: http_proxy: https_proxy: no_proxy:]\"\n",
            "time=2025-01-31T00:31:29.271Z level=INFO source=images.go:432 msg=\"total blobs: 0\"\n",
            "time=2025-01-31T00:31:29.271Z level=INFO source=images.go:439 msg=\"total unused blobs removed: 0\"\n",
            "[GIN-debug] [WARNING] Creating an Engine instance with the Logger and Recovery middleware already attached.\n",
            "\n",
            "[GIN-debug] [WARNING] Running in \"debug\" mode. Switch to \"release\" mode in production.\n",
            "- using env:\texport GIN_MODE=release\n",
            "- using code:\tgin.SetMode(gin.ReleaseMode)\n",
            "\n",
            "[GIN-debug] POST   /api/pull                 --> github.com/ollama/ollama/server.(*Server).PullHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/generate             --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/chat                 --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/embed                --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/embeddings           --> github.com/ollama/ollama/server.(*Server).EmbeddingsHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/create               --> github.com/ollama/ollama/server.(*Server).CreateHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/push                 --> github.com/ollama/ollama/server.(*Server).PushHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/copy                 --> github.com/ollama/ollama/server.(*Server).CopyHandler-fm (5 handlers)\n",
            "[GIN-debug] DELETE /api/delete               --> github.com/ollama/ollama/server.(*Server).DeleteHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/show                 --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).CreateBlobHandler-fm (5 handlers)\n",
            "[GIN-debug] HEAD   /api/blobs/:digest        --> github.com/ollama/ollama/server.(*Server).HeadBlobHandler-fm (5 handlers)\n",
            "[GIN-debug] GET    /api/ps                   --> github.com/ollama/ollama/server.(*Server).PsHandler-fm (5 handlers)\n",
            "[GIN-debug] POST   /v1/chat/completions      --> github.com/ollama/ollama/server.(*Server).ChatHandler-fm (6 handlers)\n",
            "[GIN-debug] POST   /v1/completions           --> github.com/ollama/ollama/server.(*Server).GenerateHandler-fm (6 handlers)\n",
            "[GIN-debug] POST   /v1/embeddings            --> github.com/ollama/ollama/server.(*Server).EmbedHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /v1/models                --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /v1/models/:model         --> github.com/ollama/ollama/server.(*Server).ShowHandler-fm (6 handlers)\n",
            "[GIN-debug] GET    /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
            "[GIN-debug] GET    /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
            "[GIN-debug] GET    /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
            "[GIN-debug] HEAD   /                         --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func1 (5 handlers)\n",
            "[GIN-debug] HEAD   /api/tags                 --> github.com/ollama/ollama/server.(*Server).ListHandler-fm (5 handlers)\n",
            "[GIN-debug] HEAD   /api/version              --> github.com/ollama/ollama/server.(*Server).GenerateRoutes.func2 (5 handlers)\n",
            "time=2025-01-31T00:31:29.272Z level=INFO source=routes.go:1238 msg=\"Listening on 127.0.0.1:11434 (version 0.5.7)\"\n",
            "time=2025-01-31T00:31:29.276Z level=INFO source=routes.go:1267 msg=\"Dynamic LLM libraries\" runners=\"[rocm_avx cpu cpu_avx cpu_avx2 cuda_v11_avx cuda_v12_avx]\"\n",
            "time=2025-01-31T00:31:29.280Z level=INFO source=gpu.go:226 msg=\"looking for compatible GPUs\"\n",
            "time=2025-01-31T00:31:29.564Z level=INFO source=types.go:131 msg=\"inference compute\" id=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 library=cuda variant=v12 compute=7.5 driver=12.4 name=\"Tesla T4\" total=\"14.7 GiB\" available=\"14.6 GiB\"\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"client session established\" obj=tunnels.session\n",
            "t=2025-01-31T00:31:29+0000 lvl=info msg=\"tunnel session started\" obj=tunnels.session\n",
            "t=2025-01-31T00:31:30+0000 lvl=info msg=\"started tunnel\" obj=tunnels name=command_line addr=http://localhost:11434 url=https://moccasin-just-pipefish.ngrok-free.app\n",
            "t=2025-01-31T00:32:17+0000 lvl=info msg=\"join connections\" obj=join id=51ad7cb1e683 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:34316\n",
            "[GIN] 2025/01/31 - 00:32:17 | 200 |      71.911µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:32:18 | 200 |     370.354µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T00:33:07+0000 lvl=info msg=\"join connections\" obj=join id=e2c3b75cfaaa l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:42150\n",
            "[GIN] 2025/01/31 - 00:33:07 | 200 |      43.023µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:33:08 | 404 |     415.171µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "time=2025-01-31T00:33:11.890Z level=INFO source=download.go:175 msg=\"downloading 914082ef7dd1 in 16 307 MB part(s)\"\n",
            "time=2025-01-31T00:33:38.454Z level=INFO source=download.go:175 msg=\"downloading 369ca498f347 in 1 387 B part(s)\"\n",
            "time=2025-01-31T00:33:40.024Z level=INFO source=download.go:175 msg=\"downloading 6e4c38e1172f in 1 1.1 KB part(s)\"\n",
            "time=2025-01-31T00:33:42.257Z level=INFO source=download.go:175 msg=\"downloading f4d24e9138dd in 1 148 B part(s)\"\n",
            "time=2025-01-31T00:33:44.477Z level=INFO source=download.go:175 msg=\"downloading 7404995e5021 in 1 487 B part(s)\"\n",
            "[GIN] 2025/01/31 - 00:34:05 | 200 | 56.582845967s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/pull\"\n",
            "[GIN] 2025/01/31 - 00:34:05 | 200 |   27.424737ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "time=2025-01-31T00:34:06.502Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T00:34:06.598Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.4 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T00:34:06.598Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T00:34:06.599Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 34841\"\n",
            "time=2025-01-31T00:34:06.604Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T00:34:06.604Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T00:34:06.604Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T00:34:07.143Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T00:34:07.159Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T00:34:07.160Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:34841\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "time=2025-01-31T00:34:07.359Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T00:34:13.885Z level=INFO source=server.go:594 msg=\"llama runner started in 7.28 seconds\"\n",
            "[GIN] 2025/01/31 - 00:34:13 | 200 |  7.575914154s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/generate\"\n",
            "[GIN] 2025/01/31 - 00:35:07 | 200 |  1.065678613s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "[GIN] 2025/01/31 - 00:35:14 | 200 |   40.357095ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "t=2025-01-31T00:37:17+0000 lvl=info msg=\"join connections\" obj=join id=b34457dc4c82 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:35768\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 00:37:21 | 200 |  3.522738375s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "[GIN] 2025/01/31 - 00:37:49 | 200 | 11.627723253s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "[GIN] 2025/01/31 - 00:38:23 | 200 |  9.560262381s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:38:54+0000 lvl=info msg=\"join connections\" obj=join id=7e311c5c039e l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43002\n",
            "[GIN] 2025/01/31 - 00:38:54 | 200 |      52.869µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:38:55 | 200 |     508.565µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T00:41:37+0000 lvl=info msg=\"join connections\" obj=join id=d33d588c924f l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:57152\n",
            "[GIN] 2025/01/31 - 00:41:37 | 200 |      45.211µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:41:38 | 200 |     495.147µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T00:42:28+0000 lvl=info msg=\"join connections\" obj=join id=ce3d688afbce l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51904\n",
            "[GIN] 2025/01/31 - 00:42:28 | 404 |      30.426µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/chat\"\n",
            "t=2025-01-31T00:44:06+0000 lvl=info msg=\"join connections\" obj=join id=64d2644937ca l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46702\n",
            "[GIN] 2025/01/31 - 00:44:06 | 404 |       8.257µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/chat\"\n",
            "t=2025-01-31T00:44:52+0000 lvl=info msg=\"join connections\" obj=join id=8be421cd9bc7 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:37232\n",
            "[GIN] 2025/01/31 - 00:44:52 | 200 |      36.763µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/\"\n",
            "t=2025-01-31T00:46:06+0000 lvl=info msg=\"join connections\" obj=join id=c2ea456925f5 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:34024\n",
            "[GIN] 2025/01/31 - 00:46:06 | 404 |        8.76µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/v1/chat\"\n",
            "t=2025-01-31T00:48:34+0000 lvl=info msg=\"join connections\" obj=join id=034380f0f9fd l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51078\n",
            "[GIN] 2025/01/31 - 00:48:34 | 404 |       6.869µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/v1/chat\"\n",
            "t=2025-01-31T00:48:49+0000 lvl=info msg=\"join connections\" obj=join id=36271585360a l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:44852\n",
            "[GIN] 2025/01/31 - 00:48:49 | 404 |     551.784µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:49:32+0000 lvl=info msg=\"join connections\" obj=join id=ff0a965fa1ad l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51314\n",
            "[GIN] 2025/01/31 - 00:49:32 | 200 |      37.397µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:49:32 | 200 |       500.5µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T00:49:41+0000 lvl=info msg=\"join connections\" obj=join id=cdcd5c085aeb l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:55512\n",
            "[GIN] 2025/01/31 - 00:49:41 | 404 |     405.823µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:49:50+0000 lvl=info msg=\"join connections\" obj=join id=12efa6ea9899 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:52520\n",
            "[GIN] 2025/01/31 - 00:49:50 | 200 |      81.204µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:49:50 | 200 |     497.626µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T00:50:29+0000 lvl=info msg=\"join connections\" obj=join id=d024571027b5 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:40106\n",
            "[GIN] 2025/01/31 - 00:50:29 | 200 |       37.04µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:50:29 | 200 |   40.813359ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "t=2025-01-31T00:50:43+0000 lvl=info msg=\"join connections\" obj=join id=bfbc21dfa336 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:40974\n",
            "[GIN] 2025/01/31 - 00:50:43 | 200 |      38.852µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:50:44 | 200 |     152.188µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/ps\"\n",
            "t=2025-01-31T00:50:55+0000 lvl=info msg=\"join connections\" obj=join id=46c4769ea7f8 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:45040\n",
            "[GIN] 2025/01/31 - 00:50:55 | 200 |      40.158µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:50:55 | 200 |   23.980223ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "time=2025-01-31T00:50:56.571Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T00:50:56.665Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.4 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T00:50:56.666Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T00:50:56.667Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 33553\"\n",
            "time=2025-01-31T00:50:56.667Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T00:50:56.667Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T00:50:56.667Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T00:50:56.721Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T00:50:56.729Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T00:50:56.730Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:33553\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T00:50:56.919Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T00:50:59.428Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2025/01/31 - 00:50:59 | 200 |  3.046073166s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/generate\"\n",
            "t=2025-01-31T00:51:14+0000 lvl=info msg=\"join connections\" obj=join id=6da94ae7ff0d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:52790\n",
            "[GIN] 2025/01/31 - 00:51:14 | 404 |     414.651µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "[GIN] 2025/01/31 - 00:51:40 | 200 |   25.145868ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "t=2025-01-31T00:52:33+0000 lvl=info msg=\"join connections\" obj=join id=12d656115fbc l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:54742\n",
            "[GIN] 2025/01/31 - 00:52:36 | 200 |  3.445698702s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:52:55+0000 lvl=info msg=\"join connections\" obj=join id=79cc0f31a879 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:59774\n",
            "[GIN] 2025/01/31 - 00:52:58 | 200 |  2.678608176s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:54:19+0000 lvl=info msg=\"join connections\" obj=join id=30f65daeecef l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:40300\n",
            "[GIN] 2025/01/31 - 00:54:19 | 404 |     517.221µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:54:28+0000 lvl=info msg=\"join connections\" obj=join id=82099949bc88 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43314\n",
            "[GIN] 2025/01/31 - 00:54:31 | 200 |  3.199196464s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:55:07+0000 lvl=info msg=\"join connections\" obj=join id=b47c3bf23d34 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:39046\n",
            "[GIN] 2025/01/31 - 00:55:11 | 200 |  3.613397095s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:55:35+0000 lvl=info msg=\"join connections\" obj=join id=60b9a1638d49 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:44974\n",
            "[GIN] 2025/01/31 - 00:55:38 | 200 |  3.211524519s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:57:20+0000 lvl=info msg=\"join connections\" obj=join id=aea85ea12779 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:55286\n",
            "[GIN] 2025/01/31 - 00:57:26 | 200 |  5.429865913s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:59:38+0000 lvl=info msg=\"join connections\" obj=join id=4407e9bedb14 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:48298\n",
            "[GIN] 2025/01/31 - 00:59:38 | 404 |     462.409µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T00:59:55+0000 lvl=info msg=\"join connections\" obj=join id=fc9bffaf52c1 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:56154\n",
            "[GIN] 2025/01/31 - 00:59:55 | 200 |      39.803µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 00:59:55 | 200 |     510.512µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:00:33+0000 lvl=info msg=\"join connections\" obj=join id=8bba8b30c0bb l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:54582\n",
            "time=2025-01-31T01:00:34.517Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"5.1 GiB\"\n",
            "time=2025-01-31T01:00:34.612Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.4 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:00:34.613Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=18 layers.model=33 layers.offload=18 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"7.7 GiB\" memory.required.partial=\"5.1 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2025-01-31T01:00:34.614Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 16384 --batch-size 512 --n-gpu-layers 18 --threads 1 --parallel 4 --port 40361\"\n",
            "time=2025-01-31T01:00:34.615Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:00:34.615Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:00:34.616Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:00:34.669Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:00:34.678Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:00:34.678Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:40361\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:00:34.867Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 18/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2439.52 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  2245.78 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 16384\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1152.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1176.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 159 (with bs=512), 3 (with bs=1)\n",
            "time=2025-01-31T01:00:37.380Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:00:47 | 200 | 13.667326339s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:04:07+0000 lvl=info msg=\"join connections\" obj=join id=5ef50f8e0ccb l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:53536\n",
            "[GIN] 2025/01/31 - 01:05:07 | 500 | 59.672963872s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:05:16+0000 lvl=info msg=\"join connections\" obj=join id=f1fd0108b4c1 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:50704\n",
            "[GIN] 2025/01/31 - 01:05:36 | 500 | 20.028668459s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:05:52+0000 lvl=info msg=\"join connections\" obj=join id=23ea9f951e7c l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:42964\n",
            "[GIN] 2025/01/31 - 01:05:52 | 200 |      41.856µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:05:52 | 200 |     473.754µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:06:12+0000 lvl=info msg=\"join connections\" obj=join id=85988494ecae l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:41364\n",
            "[GIN] 2025/01/31 - 01:07:11 | 500 | 59.661851219s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:07:18+0000 lvl=info msg=\"join connections\" obj=join id=0f32281275a8 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:49120\n",
            "[GIN] 2025/01/31 - 01:07:18 | 200 |      35.264µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:07:19 | 200 |     516.981µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:07:28+0000 lvl=info msg=\"join connections\" obj=join id=b3d387378a34 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:60774\n",
            "time=2025-01-31T01:07:30.218Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:07:30.368Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.2 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:07:30.371Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:07:30.373Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 45353\"\n",
            "time=2025-01-31T01:07:30.373Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:07:30.373Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:07:30.378Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:07:30.448Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:07:30.462Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:07:30.485Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:45353\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "time=2025-01-31T01:07:30.630Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:07:33.140Z level=INFO source=server.go:594 msg=\"llama runner started in 2.77 seconds\"\n",
            "[GIN] 2025/01/31 - 01:07:36 | 200 |  8.093703122s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:08:22+0000 lvl=info msg=\"join connections\" obj=join id=db3451f7709b l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:56690\n",
            "time=2025-01-31T01:08:23.003Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"5.1 GiB\"\n",
            "time=2025-01-31T01:08:23.096Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:08:23.097Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=18 layers.model=33 layers.offload=18 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"7.7 GiB\" memory.required.partial=\"5.1 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2025-01-31T01:08:23.098Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 16384 --batch-size 512 --n-gpu-layers 18 --threads 1 --parallel 4 --port 46511\"\n",
            "time=2025-01-31T01:08:23.098Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:08:23.098Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:08:23.098Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:08:23.149Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:08:23.157Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:08:23.158Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:46511\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:08:23.350Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 18/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2439.52 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  2245.78 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 16384\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1152.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1176.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 159 (with bs=512), 3 (with bs=1)\n",
            "time=2025-01-31T01:08:25.609Z level=INFO source=server.go:594 msg=\"llama runner started in 2.51 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:09:12 | 200 | 49.768838155s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:12:36+0000 lvl=info msg=\"join connections\" obj=join id=cd4337e8cbf9 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:47098\n",
            "[GIN] 2025/01/31 - 01:12:36 | 200 |      53.655µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:12:37 | 200 |     548.709µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:13:04+0000 lvl=info msg=\"join connections\" obj=join id=f0b4ea1a56f0 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:55994\n",
            "[GIN] 2025/01/31 - 01:14:04 | 500 | 59.658831708s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:14:10+0000 lvl=info msg=\"join connections\" obj=join id=3df58c8c7ed2 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:35086\n",
            "[GIN] 2025/01/31 - 01:14:10 | 200 |      44.366µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:14:10 | 200 |       560.2µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:14:21+0000 lvl=info msg=\"join connections\" obj=join id=a8e4752ac9f8 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:35548\n",
            "t=2025-01-31T01:15:01+0000 lvl=info msg=\"join connections\" obj=join id=db45018c2a4d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43144\n",
            "[GIN] 2025/01/31 - 01:15:21 | 500 | 59.653201988s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "time=2025-01-31T01:15:22.157Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:15:22.279Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:15:22.281Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:15:22.282Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 45423\"\n",
            "time=2025-01-31T01:15:22.283Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:15:22.284Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:15:22.290Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:15:22.354Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:15:22.368Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:15:22.369Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:45423\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "time=2025-01-31T01:15:22.542Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "time=2025-01-31T01:15:23.043Z level=WARN source=server.go:562 msg=\"client connection closed before server finished loading, aborting load\"\n",
            "time=2025-01-31T01:15:23.043Z level=ERROR source=sched.go:455 msg=\"error loading llama server\" error=\"timed out waiting for llama runner to start: context canceled\"\n",
            "[GIN] 2025/01/31 - 01:15:23 | 499 | 21.316311306s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "time=2025-01-31T01:15:28.244Z level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.2001786 model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc\n",
            "time=2025-01-31T01:15:28.494Z level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.450352559 model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc\n",
            "time=2025-01-31T01:15:28.743Z level=WARN source=sched.go:646 msg=\"gpu VRAM usage didn't recover within timeout\" seconds=5.699683829 model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc\n",
            "t=2025-01-31T01:15:31+0000 lvl=info msg=\"join connections\" obj=join id=93e959f8956d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43364\n",
            "time=2025-01-31T01:15:32.120Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:15:32.215Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:15:32.216Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:15:32.217Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 39665\"\n",
            "time=2025-01-31T01:15:32.217Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:15:32.217Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:15:32.218Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:15:32.268Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:15:32.276Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:15:32.276Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:39665\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:15:32.469Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:15:34.980Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2025/01/31 - 01:15:38 | 200 |  7.052688442s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:15:50+0000 lvl=info msg=\"join connections\" obj=join id=c0d9d08295d5 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43458\n",
            "time=2025-01-31T01:15:50.828Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"5.1 GiB\"\n",
            "time=2025-01-31T01:15:50.920Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:15:50.920Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=18 layers.model=33 layers.offload=18 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"7.7 GiB\" memory.required.partial=\"5.1 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2025-01-31T01:15:50.921Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 16384 --batch-size 512 --n-gpu-layers 18 --threads 1 --parallel 4 --port 41259\"\n",
            "time=2025-01-31T01:15:50.922Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:15:50.922Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:15:50.923Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:15:50.972Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:15:50.980Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:15:50.981Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:41259\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:15:51.175Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 18/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2439.52 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  2245.78 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 16384\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1152.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1176.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 159 (with bs=512), 3 (with bs=1)\n",
            "time=2025-01-31T01:15:53.685Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "t=2025-01-31T01:16:43+0000 lvl=info msg=\"join connections\" obj=join id=c77c8685905d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:48892\n",
            "[GIN] 2025/01/31 - 01:16:49 | 500 | 59.680152738s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "time=2025-01-31T01:16:50.814Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:16:50.906Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:16:50.907Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:16:50.908Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 43869\"\n",
            "time=2025-01-31T01:16:50.908Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:16:50.908Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:16:50.909Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:16:50.958Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:16:50.966Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:16:50.966Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:43869\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "time=2025-01-31T01:16:51.161Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:16:53.672Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2025/01/31 - 01:16:58 | 200 | 14.551184313s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:17:04+0000 lvl=info msg=\"join connections\" obj=join id=803ce7ed8802 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:39496\n",
            "time=2025-01-31T01:17:04.894Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"5.1 GiB\"\n",
            "time=2025-01-31T01:17:04.986Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.3 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:17:04.987Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=18 layers.model=33 layers.offload=18 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"7.7 GiB\" memory.required.partial=\"5.1 GiB\" memory.required.kv=\"2.0 GiB\" memory.required.allocations=\"[5.1 GiB]\" memory.weights.total=\"5.9 GiB\" memory.weights.repeating=\"5.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"1.1 GiB\" memory.graph.partial=\"1.1 GiB\"\n",
            "time=2025-01-31T01:17:04.988Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 16384 --batch-size 512 --n-gpu-layers 18 --threads 1 --parallel 4 --port 44171\"\n",
            "time=2025-01-31T01:17:04.988Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:17:04.988Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:17:04.989Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:17:05.039Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:17:05.048Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:17:05.049Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:44171\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:17:05.241Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 18 repeating layers to GPU\n",
            "llm_load_tensors: offloaded 18/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =  2439.52 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  2245.78 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 16384\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 16384, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:        CPU KV buffer size =   896.00 MiB\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1152.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n",
            "llama_new_context_with_model:        CPU  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =  1176.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    40.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 159 (with bs=512), 3 (with bs=1)\n",
            "time=2025-01-31T01:17:08.253Z level=INFO source=server.go:594 msg=\"llama runner started in 3.26 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:18:03 | 500 | 59.670693295s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:18:26+0000 lvl=info msg=\"join connections\" obj=join id=8efe22cb7cdc l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:49952\n",
            "t=2025-01-31T01:18:38+0000 lvl=info msg=\"join connections\" obj=join id=4dbfb74df028 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:60602\n",
            "[GIN] 2025/01/31 - 01:18:38 | 200 |      38.323µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:18:38 | 200 |     495.862µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:18:52+0000 lvl=info msg=\"join connections\" obj=join id=6c9c503498d1 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:49432\n",
            "[GIN] 2025/01/31 - 01:18:52 | 200 |      36.959µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:18:52 | 200 |   40.465617ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/show\"\n",
            "[GIN] 2025/01/31 - 01:18:57 | 500 | 31.115601937s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "time=2025-01-31T01:18:58.875Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:18:58.969Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.2 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:18:58.970Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:18:58.971Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 43279\"\n",
            "time=2025-01-31T01:18:58.971Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:18:58.971Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:18:58.971Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:18:59.023Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:18:59.032Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:18:59.032Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:43279\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "time=2025-01-31T01:18:59.223Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:19:01.732Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "[GIN] 2025/01/31 - 01:19:01 | 200 |  8.709836058s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/generate\"\n",
            "t=2025-01-31T01:19:06+0000 lvl=info msg=\"join connections\" obj=join id=3087bcae13ab l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:32940\n",
            "time=2025-01-31T01:19:07.260Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.2 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:19:07.261Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=90 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"5.7 GiB\" memory.required.partial=\"5.7 GiB\" memory.required.kv=\"512.0 MiB\" memory.required.allocations=\"[5.7 GiB]\" memory.weights.total=\"4.4 GiB\" memory.weights.repeating=\"4.0 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"296.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:19:07.262Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 4096 --batch-size 512 --n-gpu-layers 90 --threads 1 --parallel 1 --port 34931\"\n",
            "time=2025-01-31T01:19:07.262Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:19:07.262Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:19:07.263Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:19:07.320Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:19:07.329Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:19:07.329Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:34931\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "time=2025-01-31T01:19:07.515Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 1\n",
            "llama_new_context_with_model: n_ctx         = 4096\n",
            "llama_new_context_with_model: n_ctx_per_seq = 4096\n",
            "llama_new_context_with_model: n_batch       = 512\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 4096, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   512.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.50 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   296.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:19:09.774Z level=INFO source=server.go:594 msg=\"llama runner started in 2.51 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:19:31 | 200 | 25.063834652s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:20:14+0000 lvl=info msg=\"join connections\" obj=join id=0e2a62dc2c26 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:54800\n",
            "[GIN] 2025/01/31 - 01:20:29 | 200 | 14.673158837s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:21:28+0000 lvl=info msg=\"join connections\" obj=join id=2163c0d9d3e6 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:57626\n",
            "[GIN] 2025/01/31 - 01:21:48 | 200 | 19.878164554s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:22:14+0000 lvl=info msg=\"join connections\" obj=join id=fdafe6da433d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:58022\n",
            "[GIN] 2025/01/31 - 01:22:25 | 200 | 11.171345109s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:23:50+0000 lvl=info msg=\"join connections\" obj=join id=4458ebc456e5 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:58690\n",
            "[GIN] 2025/01/31 - 01:24:10 | 200 | 20.539655143s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:25:01+0000 lvl=info msg=\"join connections\" obj=join id=3602d6e556ae l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:57166\n",
            "[GIN] 2025/01/31 - 01:25:17 | 200 | 15.793307615s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:26:41+0000 lvl=info msg=\"join connections\" obj=join id=052240bf8d2c l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46348\n",
            "[GIN] 2025/01/31 - 01:26:55 | 200 | 13.672563281s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:27:37+0000 lvl=info msg=\"join connections\" obj=join id=e6f8ae3532bc l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:45240\n",
            "[GIN] 2025/01/31 - 01:27:37 | 200 |      39.979µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:27:38 | 200 |     509.193µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:30:51+0000 lvl=info msg=\"join connections\" obj=join id=907da06a10a9 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:49196\n",
            "[GIN] 2025/01/31 - 01:30:51 | 404 |       6.695µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/chat\"\n",
            "t=2025-01-31T01:31:23+0000 lvl=info msg=\"join connections\" obj=join id=cb1b9cff43bc l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43486\n",
            "[GIN] 2025/01/31 - 01:31:23 | 404 |       6.837µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/chat\"\n",
            "t=2025-01-31T01:32:44+0000 lvl=info msg=\"join connections\" obj=join id=9f6cb751e3bb l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:58022\n",
            "time=2025-01-31T01:32:45.314Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:32:45.437Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.2 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:32:45.439Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:32:45.439Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 41399\"\n",
            "time=2025-01-31T01:32:45.450Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:32:45.450Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:32:45.450Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:32:45.512Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:32:45.527Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:32:45.527Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:41399\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "time=2025-01-31T01:32:45.702Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:32:48.965Z level=INFO source=server.go:594 msg=\"llama runner started in 3.51 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:33:10 | 200 | 25.164249881s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:37:23+0000 lvl=info msg=\"join connections\" obj=join id=ca9b636e800a l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:45164\n",
            "[GIN] 2025/01/31 - 01:37:23 | 200 |     102.732µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:37:23 | 200 |       533.8µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:37:35+0000 lvl=info msg=\"join connections\" obj=join id=92286fe59661 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43366\n",
            "[GIN] 2025/01/31 - 01:38:01 | 200 | 25.380180883s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:38:58+0000 lvl=info msg=\"join connections\" obj=join id=c94cd43e4202 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:49582\n",
            "[GIN] 2025/01/31 - 01:39:39 | 200 | 41.485889991s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:44:30+0000 lvl=info msg=\"join connections\" obj=join id=abdca2c02d3c l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:33364\n",
            "[GIN] 2025/01/31 - 01:44:30 | 200 |      50.625µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:44:30 | 200 |     510.027µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:48:36+0000 lvl=info msg=\"join connections\" obj=join id=3e98d6de1253 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:53350\n",
            "time=2025-01-31T01:48:36.906Z level=INFO source=sched.go:714 msg=\"new model will fit in available VRAM in single GPU, loading\" model=/root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc gpu=GPU-cd502ded-45f3-e02c-373a-a859ca9237f6 parallel=4 available=15720382464 required=\"6.5 GiB\"\n",
            "time=2025-01-31T01:48:37.002Z level=INFO source=server.go:104 msg=\"system memory\" total=\"12.7 GiB\" free=\"11.2 GiB\" free_swap=\"0 B\"\n",
            "time=2025-01-31T01:48:37.003Z level=INFO source=memory.go:356 msg=\"offload to cuda\" layers.requested=-1 layers.model=33 layers.offload=33 layers.split=\"\" memory.available=\"[14.6 GiB]\" memory.gpu_overhead=\"0 B\" memory.required.full=\"6.5 GiB\" memory.required.partial=\"6.5 GiB\" memory.required.kv=\"1.0 GiB\" memory.required.allocations=\"[6.5 GiB]\" memory.weights.total=\"4.9 GiB\" memory.weights.repeating=\"4.5 GiB\" memory.weights.nonrepeating=\"411.0 MiB\" memory.graph.full=\"560.0 MiB\" memory.graph.partial=\"677.5 MiB\"\n",
            "time=2025-01-31T01:48:37.004Z level=INFO source=server.go:376 msg=\"starting llama server\" cmd=\"/usr/local/lib/ollama/runners/cuda_v12_avx/ollama_llama_server runner --model /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc --ctx-size 8192 --batch-size 512 --n-gpu-layers 33 --threads 1 --parallel 4 --port 36631\"\n",
            "time=2025-01-31T01:48:37.005Z level=INFO source=sched.go:449 msg=\"loaded runners\" count=1\n",
            "time=2025-01-31T01:48:37.005Z level=INFO source=server.go:555 msg=\"waiting for llama runner to start responding\"\n",
            "time=2025-01-31T01:48:37.005Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server error\"\n",
            "time=2025-01-31T01:48:37.056Z level=INFO source=runner.go:936 msg=\"starting go runner\"\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no\n",
            "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
            "ggml_cuda_init: found 1 CUDA devices:\n",
            "Device 0: Tesla T4, compute capability 7.5, VMM: yes\n",
            "time=2025-01-31T01:48:37.064Z level=INFO source=runner.go:937 msg=system info=\"CUDA : ARCHS = 600,610,620,700,720,750,800,860,870,890,900 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | LLAMAFILE = 1 | AARCH64_REPACK = 1 | cgo(gcc)\" threads=1\n",
            "time=2025-01-31T01:48:37.065Z level=INFO source=.:0 msg=\"Server listening on 127.0.0.1:36631\"\n",
            "llama_load_model_from_file: using device CUDA0 (Tesla T4) - 14992 MiB free\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "time=2025-01-31T01:48:37.257Z level=INFO source=server.go:589 msg=\"waiting for server to become available\" status=\"llm server loading model\"\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 0\n",
            "llm_load_print_meta: n_ctx_train      = 131072\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 8\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_swa            = 0\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 4\n",
            "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
            "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 14336\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: causal attn      = 1\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 500000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_ctx_orig_yarn  = 131072\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: ssm_d_conv       = 0\n",
            "llm_load_print_meta: ssm_d_inner      = 0\n",
            "llm_load_print_meta: ssm_d_state      = 0\n",
            "llm_load_print_meta: ssm_dt_rank      = 0\n",
            "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
            "llm_load_print_meta: model type       = 8B\n",
            "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading output layer to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:   CPU_Mapped model buffer size =   281.81 MiB\n",
            "llm_load_tensors:        CUDA0 model buffer size =  4403.49 MiB\n",
            "llama_new_context_with_model: n_seq_max     = 4\n",
            "llama_new_context_with_model: n_ctx         = 8192\n",
            "llama_new_context_with_model: n_ctx_per_seq = 2048\n",
            "llama_new_context_with_model: n_batch       = 2048\n",
            "llama_new_context_with_model: n_ubatch      = 512\n",
            "llama_new_context_with_model: flash_attn    = 0\n",
            "llama_new_context_with_model: freq_base     = 500000.0\n",
            "llama_new_context_with_model: freq_scale    = 1\n",
            "llama_new_context_with_model: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =  1024.00 MiB\n",
            "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host  output buffer size =     2.02 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =   560.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =    24.01 MiB\n",
            "llama_new_context_with_model: graph nodes  = 1030\n",
            "llama_new_context_with_model: graph splits = 2\n",
            "time=2025-01-31T01:48:39.767Z level=INFO source=server.go:594 msg=\"llama runner started in 2.76 seconds\"\n",
            "llama_model_loader: loaded meta data with 35 key-value pairs and 292 tensors from /root/.ollama/models/blobs/sha256-914082ef7dd1aa657de1107e8584e82f6730abe1d54dbbe1a0737cc8d70cbfbc (version GGUF V3 (latest))\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.type str              = model\n",
            "llama_model_loader: - kv   2:                               general.name str              = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llama_model_loader: - kv   3:                           general.finetune str              = abliterated\n",
            "llama_model_loader: - kv   4:                           general.basename str              = DeepSeek-R1-Distill-Llama\n",
            "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
            "llama_model_loader: - kv   6:                   general.base_model.count u32              = 1\n",
            "llama_model_loader: - kv   7:                  general.base_model.0.name str              = DeepSeek R1 Distill Llama 8B\n",
            "llama_model_loader: - kv   8:          general.base_model.0.organization str              = Deepseek Ai\n",
            "llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/deepseek-ai/De...\n",
            "llama_model_loader: - kv  10:                               general.tags arr[str,2]       = [\"abliterated\", \"uncensored\"]\n",
            "llama_model_loader: - kv  11:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv  12:                       llama.context_length u32              = 131072\n",
            "llama_model_loader: - kv  13:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv  14:                  llama.feed_forward_length u32              = 14336\n",
            "llama_model_loader: - kv  15:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv  16:              llama.attention.head_count_kv u32              = 8\n",
            "llama_model_loader: - kv  17:                       llama.rope.freq_base f32              = 500000.000000\n",
            "llama_model_loader: - kv  18:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
            "llama_model_loader: - kv  19:                           llama.vocab_size u32              = 128256\n",
            "llama_model_loader: - kv  20:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = gpt2\n",
            "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = llama-bpe\n",
            "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
            "llama_model_loader: - kv  24:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
            "llama_model_loader: - kv  25:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
            "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 128000\n",
            "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 128001\n",
            "llama_model_loader: - kv  28:            tokenizer.ggml.padding_token_id u32              = 128001\n",
            "llama_model_loader: - kv  29:               tokenizer.ggml.add_bos_token bool             = true\n",
            "llama_model_loader: - kv  30:               tokenizer.ggml.add_eos_token bool             = false\n",
            "llama_model_loader: - kv  31:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
            "llama_model_loader: - kv  32:            tokenizer.ggml.add_space_prefix bool             = false\n",
            "llama_model_loader: - kv  33:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - kv  34:                          general.file_type u32              = 15\n",
            "llama_model_loader: - type  f32:   66 tensors\n",
            "llama_model_loader: - type q4_K:  193 tensors\n",
            "llama_model_loader: - type q6_K:   33 tensors\n",
            "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
            "llm_load_vocab: special tokens cache size = 256\n",
            "llm_load_vocab: token to piece cache size = 0.7999 MB\n",
            "llm_load_print_meta: format           = GGUF V3 (latest)\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = BPE\n",
            "llm_load_print_meta: n_vocab          = 128256\n",
            "llm_load_print_meta: n_merges         = 280147\n",
            "llm_load_print_meta: vocab_only       = 1\n",
            "llm_load_print_meta: model type       = ?B\n",
            "llm_load_print_meta: model ftype      = all F32\n",
            "llm_load_print_meta: model params     = 8.03 B\n",
            "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW)\n",
            "llm_load_print_meta: general.name     = DeepSeek R1 Distill Llama 8B Abliterated\n",
            "llm_load_print_meta: BOS token        = 128000 '<｜begin▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOS token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: EOM token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: PAD token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: LF token         = 128 'Ä'\n",
            "llm_load_print_meta: EOG token        = 128001 '<｜end▁of▁sentence｜>'\n",
            "llm_load_print_meta: EOG token        = 128008 '<|eom_id|>'\n",
            "llm_load_print_meta: EOG token        = 128009 '<|eot_id|>'\n",
            "llm_load_print_meta: max token length = 256\n",
            "llama_model_load: vocab only - skipping tensors\n",
            "[GIN] 2025/01/31 - 01:49:07 | 200 | 30.660960701s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:51:27+0000 lvl=info msg=\"join connections\" obj=join id=072ed775a1a4 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:52062\n",
            "[GIN] 2025/01/31 - 01:51:54 | 200 | 27.275615765s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:52:57+0000 lvl=info msg=\"join connections\" obj=join id=fb011a10eddd l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:42386\n",
            "[GIN] 2025/01/31 - 01:53:57 | 500 | 59.665106726s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:54:12+0000 lvl=info msg=\"join connections\" obj=join id=8c1ef37a37e3 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51850\n",
            "[GIN] 2025/01/31 - 01:54:12 | 200 |      37.606µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:54:12 | 200 |     489.123µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:54:27+0000 lvl=info msg=\"join connections\" obj=join id=423d66a29c0e l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:48272\n",
            "[GIN] 2025/01/31 - 01:54:32 | 200 |  4.766254129s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:54:51+0000 lvl=info msg=\"join connections\" obj=join id=651fb13f634e l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:45032\n",
            "t=2025-01-31T01:55:15+0000 lvl=info msg=\"join connections\" obj=join id=e6da0a16c7ab l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46288\n",
            "[GIN] 2025/01/31 - 01:55:15 | 200 |      39.637µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:55:16 | 200 |     439.039µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "[GIN] 2025/01/31 - 01:55:50 | 500 | 59.687356904s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:55:58+0000 lvl=info msg=\"join connections\" obj=join id=9c5f3f94ed55 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:47746\n",
            "[GIN] 2025/01/31 - 01:55:58 | 200 |      43.407µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 01:55:58 | 200 |     432.961µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T01:56:03+0000 lvl=info msg=\"join connections\" obj=join id=32beedd3777c l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:37918\n",
            "[GIN] 2025/01/31 - 01:56:07 | 200 |  4.120393947s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:56:20+0000 lvl=info msg=\"join connections\" obj=join id=3bcd11289d9e l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:50392\n",
            "[GIN] 2025/01/31 - 01:56:42 | 200 | 22.045707109s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T01:58:39+0000 lvl=info msg=\"join connections\" obj=join id=4bddd6acc56d l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:43198\n",
            "[GIN] 2025/01/31 - 01:59:17 | 200 | 37.958137277s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:01:48+0000 lvl=info msg=\"join connections\" obj=join id=8b56425b122a l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:38546\n",
            "[GIN] 2025/01/31 - 02:01:48 | 200 |       54.93µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 02:01:49 | 200 |     456.393µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T02:02:38+0000 lvl=info msg=\"join connections\" obj=join id=dfbc718683cf l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:32814\n",
            "[GIN] 2025/01/31 - 02:02:59 | 200 | 20.157809523s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:05:38+0000 lvl=info msg=\"join connections\" obj=join id=a7cff9837a6e l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:56072\n",
            "[GIN] 2025/01/31 - 02:06:13 | 200 | 34.990163458s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:09:05+0000 lvl=info msg=\"join connections\" obj=join id=b7a7a2bc1edd l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:39784\n",
            "[GIN] 2025/01/31 - 02:09:45 | 200 | 40.284872196s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:12:12+0000 lvl=info msg=\"join connections\" obj=join id=320dc3b6daf5 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:45226\n",
            "[GIN] 2025/01/31 - 02:12:41 | 200 | 29.064361927s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:13:00+0000 lvl=info msg=\"join connections\" obj=join id=676bfe04fc19 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:52858\n",
            "[GIN] 2025/01/31 - 02:13:00 | 200 |      38.706µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 02:13:00 | 200 |     525.878µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T02:16:37+0000 lvl=info msg=\"join connections\" obj=join id=b09275511ae7 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46292\n",
            "[GIN] 2025/01/31 - 02:17:00 | 200 | 22.898553015s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:18:28+0000 lvl=info msg=\"join connections\" obj=join id=725873e273e0 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46408\n",
            "[GIN] 2025/01/31 - 02:19:27 | 500 | 59.580411782s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:20:02+0000 lvl=info msg=\"join connections\" obj=join id=592a3f4a67fb l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:39026\n",
            "[GIN] 2025/01/31 - 02:20:02 | 200 |      42.227µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | HEAD     \"/\"\n",
            "[GIN] 2025/01/31 - 02:20:02 | 200 |     561.486µs | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | GET      \"/api/tags\"\n",
            "t=2025-01-31T02:20:16+0000 lvl=info msg=\"join connections\" obj=join id=8cc8a47d08e9 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:48660\n",
            "[GIN] 2025/01/31 - 02:20:16 | 200 |  657.691214ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "[GIN] 2025/01/31 - 02:20:28 | 200 |  4.579636583s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:23:52+0000 lvl=info msg=\"join connections\" obj=join id=901ac5a04ed8 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51120\n",
            "[GIN] 2025/01/31 - 02:23:52 | 200 |  801.965442ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:25:03+0000 lvl=info msg=\"join connections\" obj=join id=fef5818b5197 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:51988\n",
            "[GIN] 2025/01/31 - 02:25:04 | 200 |  629.083463ms | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:26:19+0000 lvl=info msg=\"join connections\" obj=join id=5e34eb2f8591 l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:46016\n",
            "[GIN] 2025/01/31 - 02:26:21 | 200 |  1.508152418s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n",
            "t=2025-01-31T02:26:49+0000 lvl=info msg=\"join connections\" obj=join id=55d3a206fc4c l=127.0.0.1:11434 r=[2803:c600:9104:c485:bc0e:cf52:f18a:1d81]:50694\n",
            "[GIN] 2025/01/31 - 02:26:59 | 200 |  9.774848884s | 2803:c600:9104:c485:bc0e:cf52:f18a:1d81 | POST     \"/api/chat\"\n"
          ]
        }
      ],
      "source": [
        "# Run multiple tasks concurrently:\n",
        "#  1. Start the Ollama server.\n",
        "#  2. Start ngrok to forward HTTP traffic from the local ollama api running on localhost:11434.\n",
        "#     Instructions come from Ollama doc: https://github.com/ollama/ollama/blob/main/docs/faq.md#how-can-i-use-ollama-with-ngrok\n",
        "await asyncio.gather(\n",
        "    run(['ollama', 'serve']),\n",
        "\n",
        "    # If you don't want to map to a static URL in Ngrok, uncomment line 9 and comment line 10 before running this cell\n",
        "    # run(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434']),\n",
        "    run(['ngrok', 'http', '--log', 'stderr', '11434', '--host-header', 'localhost:11434', '--domain', 'reemplazacontudomainde.ngrok-free.app']),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}